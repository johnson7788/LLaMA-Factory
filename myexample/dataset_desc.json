{
    "alpaca_en": {
        "desc": "chatgpt生成的alpaca数据集",
        "url": "https://huggingface.co/datasets/QingyiSi/Alpaca-CoT",
        "paper": "",
        "format": "alpaca",
        "language": "英文",
        "multiturn": false,
        "length": 52000,
        "formodel": "Pretrain,SFT"
    },
    "alpaca_zh": {
        "desc": "chatgpt生成的alpaca数据集",
        "url": "https://huggingface.co/datasets/QingyiSi/Alpaca-CoT",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": false,
        "length": 52000,
        "formodel": "Pretrain,SFT"
    },
    "alpaca_gpt4_en": {
        "desc": "gpt4生成的alpaca数据集",
        "url": "https://huggingface.co/datasets/QingyiSi/Alpaca-CoT",
        "paper": "",
        "format": "alpaca",
        "language": "英文",
        "multiturn": false,
        "length": 52000,
        "formodel": "Pretrain,SFT"
    },
    "alpaca_gpt4_zh": {
        "desc": "gpt4生成的alpaca数据集",
        "url": "https://huggingface.co/datasets/QingyiSi/Alpaca-CoT",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": false,
        "length": 52000,
        "formodel": "Pretrain,SFT"
    },
    "self_cognition": {
        "desc": "带参数的模型自我介绍数据集",
        "url": "",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": false,
        "length": 80,
        "formodel": "SFT"
    },
    "oaast_sft": {
        "desc": "OpenAssistant Conversations对话数据集",
        "url": "https://huggingface.co/datasets/OpenAssistant/oasst1",
        "paper": "OpenAssistant Conversations -- Democratizing Large Language Model Alignment",
        "format": "alpaca",
        "language": "英文",
        "multiturn": true,
        "length": 20202,
        "formodel": "Pretrain,SFT"
    },
    "oaast_sft_zh": {
        "desc": "OpenAssistant Conversations中文对话数据集",
        "url": "https://huggingface.co/datasets/OpenAssistant/oasst1",
        "paper": "OpenAssistant Conversations -- Democratizing Large Language Model Alignment",
        "format": "alpaca",
        "language": "中文",
        "multiturn": true,
        "length": 689,
        "formodel": "Pretrain,SFT"
    },
    "lima": {
        "desc": "对齐数据集，对话数据集",
        "url": "https://huggingface.co/datasets/GAIR/lima",
        "paper": "LIMA: Less Is More for Alignment",
        "format": "alpaca",
        "language": "英文",
        "multiturn": true,
        "length": 1029,
        "formodel": "Pretrain,SFT"
    },
    "glaive_toolcall": {
        "desc": "glaive公司发布的工具调用问答数据集,有human，function_call，observation，gpt，tools组成",
        "url": "https://huggingface.co/datasets/glaiveai/glaive-function-calling",
        "paper": "",
        "format": "sharegpt",
        "language": "英文",
        "multiturn": true,
        "length": 10000,
        "formodel": "Pretrain,SFT"
    },
    "example": {
        "desc": "示例数据集，只有2条",
        "url": "",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": true,
        "length": 2,
        "formodel": "Pretrain,SFT"
    },
    "guanaco": {
        "desc": "原驼数据集",
        "url": "https://huggingface.co/datasets/JosephusCheung/GuanacoDataset",
        "paper": "",
        "format": "alpaca",
        "language": "中文，英文，日文",
        "multiturn": false,
        "length": 534530,
        "formodel": "Pretrain,SFT"
    },
    "belle_2m": {
        "desc": "Belle发布的200万对话数据集",
        "url": "https://huggingface.co/datasets/BelleGroup/train_2M_CN",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": false,
        "length": 2000000,
        "formodel": "Pretrain,SFT"
    },
    "belle_1m": {
        "desc": "100万条由BELLE项目生成的中文指令数据",
        "url": "https://huggingface.co/datasets/BelleGroup/train_1M_CN",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": false,
        "length": 1000000,
        "formodel": "Pretrain,SFT"
    },
    "belle_0.5m": {
        "desc": "50万条由BELLE项目生成的中文指令数据。",
        "url": "https://huggingface.co/datasets/BelleGroup/train_0.5M_CN",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": false,
        "length": 500000,
        "formodel": "Pretrain,SFT"
    },
    "belle_dialog": {
        "desc": "40万条由BELLE项目生成的个性化角色对话数据",
        "url": "https://huggingface.co/datasets/BelleGroup/generated_chat_0.4M?row=1",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": true,
        "length": 400000,
        "formodel": "Pretrain,SFT"
    },
    "belle_math": {
        "desc": "25万条由BELLE项目生成的中文数学题数据，包含解题过程,由ChatGPT产生的",
        "url": "https://huggingface.co/datasets/BelleGroup/school_math_0.25M",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": false,
        "length": 250000,
        "formodel": "Pretrain,SFT"
    },
    "belle_multiturn": {
        "desc": "包含约80万条由BELLE项目生成的用户与助手的多轮对话",
        "url": "https://huggingface.co/datasets/BelleGroup/multiturn_chat_0.8M",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": true,
        "length": 800000,
        "formodel": "Pretrain,SFT"
    },
    "ultra_chat": {
        "desc": "Turbo API 提供支持的开源、大规模、多轮对话数据,采用了两个独立的 ChatGPT Turbo API",
        "url": "https://huggingface.co/datasets/stingning/ultrachat",
        "paper": "",
        "format": "alpaca",
        "language": "英文",
        "multiturn": true,
        "length": 770000,
        "formodel": "Pretrain,SFT"
    },
    "open_platypus": {
        "desc": "专注于提高LLM逻辑推理能力",
        "url": "https://huggingface.co/datasets/garage-bAInd/Open-Platypus",
        "paper": "Platypus: Quick, Cheap, and Powerful Refinement of LLMs",
        "format": "alpaca",
        "language": "英文",
        "multiturn": false,
        "length": 249000,
        "formodel": "Pretrain,SFT"
    },
    "codealpaca": {
        "desc": "根据问题生成python代码",
        "url": "https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k",
        "paper": "",
        "format": "alpaca",
        "language": "英文",
        "multiturn": false,
        "length": 20000,
        "formodel": "Pretrain,SFT"
    },
    "alpaca_cot": {
        "desc": "指令调优数据集",
        "url": "https://huggingface.co/datasets/QingyiSi/Alpaca-CoT",
        "paper": "",
        "format": "alpaca",
        "language": "英文",
        "multiturn": false,
        "length": 74771,
        "formodel": "Pretrain,SFT"
    },
    "openorca": {
        "desc": "OpenOrca 数据集是增强型 FLAN 集合数据的集合。目前 GPT-4 完成量约为 100 万，GPT-3.5 完成量约为 320 万,带GPT4解释轨迹的",
        "url": "https://huggingface.co/datasets/Open-Orca/OpenOrca",
        "paper": "Orca: Progressive Learning from Complex Explanation Traces of GPT-4",
        "format": "alpaca",
        "language": "英文",
        "multiturn": false,
        "length": 4200000,
        "formodel": "Pretrain,SFT"
    },
    "mathinstruct": {
        "desc": "精心策划的指令调整数据集， 由 13 个数学原理数据集编译而成，关注思想链（CoT）和思想程序（PoT）原理的混合使用",
        "url": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct",
        "paper": "MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning",
        "format": "alpaca",
        "language": "英文",
        "multiturn": false,
        "length": 262000,
        "formodel": "Pretrain,SFT"
    },
    "firefly": {
        "desc": "流萤中文对话式大语言模型，23个常见的中文数据集",
        "url": "https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": false,
        "length": 1150000,
        "formodel": "Pretrain,SFT"
    },
    "webqa": {
        "desc": "指令微调数据集",
        "url": "https://huggingface.co/datasets/suolyer/webqa",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": false,
        "length": 36200,
        "formodel": "Pretrain,SFT"
    },
    "webnovel": {
        "desc": "包含从12560本网文提取的约21.7M条可用于训练小说生成的中文指令数据，质量不好",
        "url": "https://huggingface.co/datasets/zxbsmk/webnovel_cn?row=3",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": false,
        "length": 21700000,
        "formodel": "Pretrain,SFT"
    },
    "nectar_sft": {
        "desc": "chatgpt生成的数据，质量一般",
        "url": "https://huggingface.co/datasets/AstraMindAI/SFT-Nectar",
        "paper": "",
        "format": "alpaca",
        "language": "英文",
        "multiturn": false,
        "length": 131000,
        "formodel": "Pretrain,SFT"
    },
    "deepctrl": {
        "desc": "匠数大模型SFT数据集",
        "url": "https://www.modelscope.cn/datasets/deepctrl/deepctrl-sft-data/summary",
        "paper": "",
        "format": "alpaca",
        "language": "英文，中文",
        "multiturn": true,
        "length": 120000000,
        "formodel": "Pretrain,SFT"
    },
    "adgen": {
        "desc": "词语生成句子数据集",
        "url": "https://huggingface.co/datasets/HasturOfficial/adgen",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": false,
        "length": 115000,
        "formodel": "SFT"
    },
    "sharegpt_hyper": {
        "desc": "Share GPT收集到的多轮对话数据",
        "url": "https://huggingface.co/datasets/totally-not-an-llm/sharegpt-hyperfiltered-3k",
        "paper": "",
        "format": "sharegpt",
        "language": "英文",
        "multiturn": true,
        "length": 3243,
        "formodel": "Pretrain,SFT"
    },
    "sharegpt4": {
        "desc": "",
        "url": "https://huggingface.co/datasets/shibing624/sharegpt_gpt4",
        "paper": "",
        "format": "sharegpt",
        "language": "英文,中文,日文",
        "multiturn": true,
        "length": 103415,
        "formodel": "Pretrain,SFT"
    },
    "ultrachat_200k": {
        "desc": "用于训练 Zephyr-7B-β的数据集",
        "url": "https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k",
        "paper": "Zephyr: Direct Distillation of LM Alignment",
        "format": "sharegpt",
        "language": "英文",
        "multiturn": true,
        "length": 200000,
        "formodel": "Pretrain,SFT"
    },
    "agent_instruct": {
        "desc": "利用GPT4生成的具有ReAct的轨迹数据集，多轮的COT",
        "url": "https://huggingface.co/datasets/THUDM/AgentInstruct",
        "paper": "",
        "format": "sharegpt",
        "language": "英文",
        "multiturn": true,
        "length": 1866,
        "formodel": "Pretrain,SFT"
    },
    "lmsys_chat": {
        "desc": "大规模现实世界LLM对话数据集，包含 100 万个真实世界对话，其中包含 25 个最先进的 LLMs",
        "url": "https://huggingface.co/datasets/lmsys/lmsys-chat-1m",
        "paper": "",
        "format": "sharegpt",
        "language": "英文",
        "multiturn": true,
        "length": 1000000,
        "formodel": "Pretrain,SFT"
    },
    "evol_instruct": {
        "desc": "数学模型数据集",
        "url": "https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k?row=0",
        "paper": "",
        "format": "sharegpt",
        "language": "英文",
        "multiturn": false,
        "length": 143000,
        "formodel": "Pretrain,SFT"
    },
    "hh_rlhf_en": {
        "desc": "reward数据集,有关有用和无害的人类偏好数据，来自通过人类反馈进行强化学习来训练有用且无害的助手",
        "url": "https://huggingface.co/datasets/Anthropic/hh-rlhf",
        "paper": "",
        "format": "alpaca",
        "language": "英文",
        "multiturn": false,
        "length": 161000,
        "formodel": "Pretrain,SFT"
    },
    "oaast_rm": {
        "desc": "训练reward模型的数据集，output中的多条回答越往后，评分越低",
        "url": "https://huggingface.co/datasets/OpenAssistant/oasst1",
        "paper": "",
        "format": "alpaca",
        "language": "英文",
        "multiturn": true,
        "length": 19122,
        "formodel": "Pretrain,SFT"
    },
    "oaast_rm_zh": {
        "desc": "训练reward模型的数据集，output中的多条回答越往后，评分越低",
        "url": "https://huggingface.co/datasets/OpenAssistant/oasst1",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": true,
        "length": 775,
        "formodel": "Pretrain,SFT"
    },
    "comparison_gpt4_en": {
        "desc": "gpt4生成的奖励模型训练集",
        "url": "https://huggingface.co/datasets/wangrongsheng/comparison_gpt4_data",
        "paper": "",
        "format": "alpaca",
        "language": "英文",
        "multiturn": false,
        "length": 36441,
        "formodel": "Pretrain,SFT"
    },
    "comparison_gpt4_zh": {
        "desc": "gpt4生成的奖励模型训练集",
        "url": "https://huggingface.co/datasets/wangrongsheng/comparison_gpt4_data",
        "paper": "",
        "format": "alpaca",
        "language": "中文",
        "multiturn": false,
        "length": 36441,
        "formodel": "Pretrain,SFT"
    },
    "nectar_rm": {
        "desc": "奖励模型训练数据集",
        "url": "https://huggingface.co/datasets/AstraMindAI/RLAIF-Nectar",
        "paper": "",
        "format": "alpaca",
        "language": "英文",
        "multiturn": false,
        "length": 763000,
        "formodel": "Pretrain,SFT"
    },
    "wiki_demo": {
        "desc": "预训练wiki数据集",
        "url": "",
        "paper": "",
        "format": "text",
        "language": "英文",
        "multiturn": false,
        "length": 1000,
        "formodel": "Pretrain"
    },
    "c4_demo": {
        "desc": "预训练数据集",
        "url": "",
        "paper": "",
        "format": "text",
        "language": "英文",
        "multiturn": false,
        "length": 300,
        "formodel": "Pretrain"
    },
    "refinedweb": {
        "desc": "Falcon RefinedWeb 通过CommonCrawl的严格过滤和大规模去重而构建的预训练数据集",
        "url": "https://huggingface.co/datasets/tiiuae/falcon-refinedweb?row=6",
        "paper": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
        "format": "text",
        "language": "英文",
        "multiturn": false,
        "length": 968000000,
        "formodel": "Pretrain"
    },
    "redpajama_v2": {
        "desc": "基于 Common Crawl 提供的 84 个快照",
        "url": "https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2",
        "paper": "RedPajama: an Open Dataset for Training Large Language Models",
        "format": "text",
        "language": "英文",
        "multiturn": false,
        "length": 470000,
        "formodel": "Pretrain"
    },
    "wikipedia_en": {
        "desc": "英文维基百科数据集",
        "url": "https://huggingface.co/datasets/olm/olm-wikipedia-20221220",
        "paper": "",
        "format": "text",
        "language": "英文",
        "multiturn": false,
        "length": 6590000,
        "formodel": "Pretrain"
    },
    "wikipedia_zh": {
        "desc": "中文维基2023年7月20日的dump存档的预训练数据集",
        "url": "https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered",
        "paper": "",
        "format": "text",
        "language": "中文",
        "multiturn": false,
        "length": 254547,
        "formodel": "Pretrain"
    },
    "pile": {
        "desc": "预训练数据集",
        "url": "https://huggingface.co/datasets/EleutherAI/pile",
        "paper": "",
        "format": "text",
        "language": "英文",
        "multiturn": false,
        "length": 8520000,
        "formodel": "Pretrain"
    },
    "skypile": {
        "desc": "新闻数据集",
        "url": "https://huggingface.co/datasets/Skywork/SkyPile-150B",
        "paper": "",
        "format": "text",
        "language": "中文",
        "multiturn": false,
        "length": 1760000,
        "formodel": "Pretrain"
    },
    "the_stack": {
        "desc": "编程语言训练",
        "url": "https://huggingface.co/datasets/bigcode/the-stack",
        "paper": "",
        "format": "text",
        "language": "英文",
        "multiturn": false,
        "length": 546000000,
        "formodel": "Pretrain"
    },
    "starcoder_python": {
        "desc": "python编程数据集",
        "url": "https://huggingface.co/datasets/bigcode/starcoderdata",
        "paper": "",
        "format": "text",
        "language": "英文",
        "multiturn": false,
        "length": 207000000,
        "formodel": "Pretrain"
    }
}